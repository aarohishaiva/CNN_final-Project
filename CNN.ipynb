{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPb/fzYdbQ58U8+pi7GntTT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aarohishaiva/CNN_final-Project/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Generate a Synthetic Dataset\n",
        "We'll create a dataset where each source sequence is a random sequence of integers, and each target sequence is the reverse of the source sequence."
      ],
      "metadata": {
        "id": "P5B8rkIPPSEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ReverseDataset(Dataset):\n",
        "    def __init__(self, num_samples=10000, seq_length=10, vocab_size=100):\n",
        "        self.num_samples = num_samples\n",
        "        self.seq_length = seq_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = self._generate_data()\n",
        "\n",
        "    def _generate_data(self):\n",
        "        data = []\n",
        "        for _ in range(self.num_samples):\n",
        "            source = np.random.randint(1, self.vocab_size, self.seq_length)\n",
        "            target = source[::-1]\n",
        "            data.append((source, target))\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "dataset = ReverseDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "3oonCNwwPWGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Implement Seq2Seq Model with Attention Mechanism\n",
        "We'll define the Encoder, Decoder, and Attention mechanisms using PyTorch."
      ],
      "metadata": {
        "id": "PIJFoYSvPl-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hid_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear((hid_dim * 2) + hid_dim, hid_dim)\n",
        "        self.v = nn.Linear(hid_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        hidden = hidden[-1].repeat(src_len, 1, 1)\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        return torch.softmax(attention, dim=0)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM((hid_dim * 2) + emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "        self.fc_out = nn.Linear((hid_dim * 2) + hid_dim + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell, encoder_outputs):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        a = self.attention(hidden, encoder_outputs)\n",
        "        a = a.unsqueeze(1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
        "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        trg_len = trg.shape[0]\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        encoder_outputs, hidden, cell = self.encoder(src)\n",
        "        input = trg[0, :]\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if np.random.random() < teacher_forcing_ratio else top1\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "4lF1FlLwPnB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Train the Model\n",
        "We'll train the model using the synthetic dataset."
      ],
      "metadata": {
        "id": "g6j2FLFfPvQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "\n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "\n",
        "        # Repeat hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # [batch size, src len, dec_hid_dim]\n",
        "\n",
        "        # Concatenate hidden state with encoder outputs\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [batch size, src len, dec_hid_dim]\n",
        "\n",
        "        attention = self.v(energy).squeeze(2)  # [batch size, src len]\n",
        "\n",
        "        return F.softmax(attention, dim=1)  # [batch size, src len]\n"
      ],
      "metadata": {
        "id": "1q96ryuaaNbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        input = input.unsqueeze(0)  # [1, batch size]\n",
        "        embedded = self.dropout(self.embedding(input))  # [1, batch size, emb dim]\n",
        "\n",
        "        a = self.attention(hidden, encoder_outputs)  # [batch size, src len]\n",
        "        a = a.unsqueeze(1)  # [batch size, 1, src len]\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # [batch size, src len, enc_hid_dim * 2]\n",
        "\n",
        "        weighted = torch.bmm(a, encoder_outputs)  # [batch size, 1, enc_hid_dim * 2]\n",
        "        weighted = weighted.permute(1, 0, 2)  # [1, batch size, enc_hid_dim * 2]\n",
        "\n",
        "        rnn_input = torch.cat((embedded, weighted), dim=2)  # [1, batch size, (enc_hid_dim * 2) + emb_dim]\n",
        "\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "\n",
        "        embedded = embedded.squeeze(0)  # [batch size, emb dim]\n",
        "        output = output.squeeze(0)  # [batch size, dec_hid_dim]\n",
        "        weighted = weighted.squeeze(0)  # [batch size, enc_hid_dim * 2]\n",
        "\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))  # [batch size, output dim]\n",
        "\n",
        "        return prediction, hidden.squeeze(0), a.squeeze(1)\n"
      ],
      "metadata": {
        "id": "PfsDMsC3aPbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "        input = trg[0, :]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            input = output.argmax(1)  # Next input is current output\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "pzhrdS4daVHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "W_VhI-B3eQgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = ['word1', 'word2', 'word3', ...]  # Replace with your actual vocabulary\n",
        "INPUT_DIM = len(vocab)\n",
        "OUTPUT_DIM = len(vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "INPUT_DIM = len(vocab)\n",
        "OUTPUT_DIM = len(vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "XibDAHfXeSMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YourDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self):\n",
        "        # Initialize your dataset here\n",
        "        self.data = None  # Replace with your actual data\n",
        "        pass\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of samples\n",
        "        return len(self.data)  # Return the length of your data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return a sample from your dataset\n",
        "        pass"
      ],
      "metadata": {
        "id": "vGolhSPkegd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        # Define your attention mechanism\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # Implement your attention mechanism forward pass\n",
        "        pass\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        input = input.unsqueeze(0)  # input shape: [1, batch size]\n",
        "        embedded = self.dropout(self.embedding(input))  # embedded shape: [1, batch size, emb dim]\n",
        "\n",
        "        a = self.attention(hidden, encoder_outputs)  # attention shape: [batch size, src len]\n",
        "        a = a.unsqueeze(1)  # attention shape: [batch size, 1, src len]\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # encoder_outputs shape: [batch size, src len, enc_hid_dim * 2]\n",
        "\n",
        "        weighted = torch.bmm(a, encoder_outputs)  # weighted shape: [batch size, 1, enc_hid_dim * 2]\n",
        "        weighted = weighted.permute(1, 0, 2)  # weighted shape: [1, batch size, enc_hid_dim * 2]\n",
        "\n",
        "        rnn_input = torch.cat((embedded, weighted), dim=2)  # rnn_input shape: [1, batch size, (enc_hid_dim * 2) + emb_dim]\n",
        "\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "\n",
        "        embedded = embedded.squeeze(0)  # embedded shape: [batch size, emb dim]\n",
        "        output = output.squeeze(0)  # output shape: [batch size, dec_hid_dim]\n",
        "        weighted = weighted.squeeze(0)  # weighted shape: [batch size, enc_hid_dim * 2]\n",
        "\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))  # prediction shape: [batch size, output_dim]\n",
        "\n",
        "        return prediction, hidden.squeeze(0), a.squeeze(1)\n"
      ],
      "metadata": {
        "id": "ZiM5mZ9neo9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch_idx, (src, trg) in enumerate(iterator):\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output, _, _ = model(src, trg[:,:-1])  # Pass trg without the last token for teacher forcing\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)  # Shifted target\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n"
      ],
      "metadata": {
        "id": "TAVMOj7petZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "tensor1 = torch.randn(3)\n",
        "tensor2 = torch.randn(3) # Change the shape of tensor2 to match tensor1\n",
        "\n",
        "stacked = torch.stack([tensor1, tensor2])\n",
        "print(stacked)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "in1ci-8mgYAZ",
        "outputId": "faea353c-9054-48a4-e0f3-99c219a36957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.5483,  1.4566,  0.9874],\n",
            "        [-1.8238, -0.2973,  0.0449]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Example of padding sequences to the same length\n",
        "tensor1 = torch.randn(3, 4)  # Example tensor 1\n",
        "tensor2 = torch.randn(2, 4)  # Example tensor 2\n",
        "\n",
        "# Pad sequences to the same length (e.g., 3)\n",
        "padded_tensor1 = pad_sequence([tensor1, tensor2], batch_first=True, padding_value=0)\n"
      ],
      "metadata": {
        "id": "srVpWKBmg9WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor1 = torch.randn(3, 4)\n",
        "tensor2 = torch.randn(3, 4)\n",
        "print(tensor1.shape, tensor2.shape)  # Check shapes before stacking\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9P1nSkgg_P5",
        "outputId": "d2dc4fd1-6e93-40e5-98a3-bc0711d017da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 4]) torch.Size([3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "tensor1 = torch.randn(3, 4)  # Shape [3, 4]\n",
        "tensor2 = torch.randn(3, 4)  # Shape [3, 4]\n",
        "\n",
        "tensor2_reshaped = tensor2.unsqueeze(0)  # Shape [1, 3, 4]\n",
        "\n",
        "# Now both tensors have the same shape [1, 3, 4] along dim=0\n",
        "stacked = torch.cat([tensor1.unsqueeze(0), tensor2_reshaped], dim=0)\n",
        "print(stacked.shape)  # Output: torch.Size([2, 3, 4])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpqSLnrUhayV",
        "outputId": "e16e3fed-4ced-4db9-9cdf-079e1085d7d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        return F.softmax(attention, dim=1)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim, bidirectional=True)\n",
        "        self.fc = nn.Linear(hid_dim * 2, hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
        "        return outputs, hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        input = input.unsqueeze(0)  # input shape: [1, batch size]\n",
        "        embedded = self.dropout(self.embedding(input))  # embedded shape: [1, batch size, emb dim]\n",
        "        a = self.attention(hidden, encoder_outputs)  # attention shape: [batch size, src len]\n",
        "        a = a.unsqueeze(1)  # attention shape: [batch size, 1, src len]\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # encoder_outputs shape: [batch size, src len, enc_hid_dim * 2]\n",
        "        weighted = torch.bmm(a, encoder_outputs)  # weighted shape: [batch size, 1, enc_hid_dim * 2]\n",
        "        weighted = weighted.permute(1, 0, 2)  # weighted shape: [1, batch size, enc_hid_dim * 2]\n",
        "        rnn_input = torch.cat((embedded, weighted), dim=2)  # rnn_input shape: [1, batch size, (enc_hid_dim * 2) + emb_dim]\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        embedded = embedded.squeeze(0)  # embedded shape: [batch size, emb dim]\n",
        "        output = output.squeeze(0)  # output shape: [batch size, dec_hid_dim]\n",
        "        weighted = weighted.squeeze(0)  # weighted shape: [batch size, enc_hid_dim * 2]\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))  # prediction shape: [batch size, output_dim]\n",
        "        return prediction, hidden.squeeze(0), a.squeeze(1)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        trg_len = trg.shape[0]\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "        input = trg[0,:]\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if random.random() < teacher_forcing_ratio else top1\n",
        "        return outputs\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for i, (src, trg) in enumerate(iterator):\n",
        "        src, trg = src.to(model.device), trg.to(model.device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Ensure all indices are within the valid range\n",
        "        src = torch.clamp(src, 0, INPUT_DIM - 1)\n",
        "        trg = torch.clamp(trg, 0, OUTPUT_DIM - 1)\n",
        "\n",
        "        output = model(src, trg)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "# Example batch size\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Custom collate function for DataLoader\n",
        "def collate_fn(batch):\n",
        "    srcs = [torch.tensor(item[0].copy()) for item in batch]\n",
        "    trgs = [torch.tensor(item[1].copy()) for item in batch]\n",
        "    srcs = pad_sequence(srcs, batch_first=True, padding_value=0)\n",
        "    trgs = pad_sequence(trgs, batch_first=True, padding_value=0)\n",
        "    return srcs, trgs\n",
        "\n",
        "# Example dataset\n",
        "# Replace with your actual dataset\n",
        "class ExampleDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self):\n",
        "        self.data = [\n",
        "            (np.array([1, 2, 3]), np.array([1, 2, 3])),\n",
        "            (np.array([4, 5]), np.array([4, 5]))\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "train_data = ExampleDataset()\n",
        "\n",
        "# DataLoader instantiation\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "# Define your vocabulary here\n",
        "vocab = {'<PAD>': 0, '<SOS>': 1, '}': 2}\n",
        "\n",
        "INPUT_DIM = len(vocab)\n",
        "OUTPUT_DIM = len(vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, ENC_DROPOUT)\n",
        "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gev2yreD3Clj",
        "outputId": "ee8fd866-4c13-41c6-b0a7-484cb5900169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Train Loss: 1.094\n",
            "Epoch: 02, Train Loss: 0.395\n",
            "Epoch: 03, Train Loss: 0.098\n",
            "Epoch: 04, Train Loss: 0.037\n",
            "Epoch: 05, Train Loss: 0.009\n",
            "Epoch: 06, Train Loss: 0.003\n",
            "Epoch: 07, Train Loss: 0.000\n",
            "Epoch: 08, Train Loss: 0.000\n",
            "Epoch: 09, Train Loss: 0.000\n",
            "Epoch: 10, Train Loss: 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Evaluate the Model\n",
        "We'll evaluate the model using accuracy and loss metrics.\n"
      ],
      "metadata": {
        "id": "cmWIF4Gd3z39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        return F.softmax(attention, dim=1)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim, bidirectional=True)\n",
        "        self.fc = nn.Linear(hid_dim * 2, hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
        "        return outputs, hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        input = input.unsqueeze(0)  # input shape: [1, batch size]\n",
        "        embedded = self.dropout(self.embedding(input))  # embedded shape: [1, batch size, emb dim]\n",
        "        a = self.attention(hidden, encoder_outputs)  # attention shape: [batch size, src len]\n",
        "        a = a.unsqueeze(1)  # attention shape: [batch size, 1, src len]\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # encoder_outputs shape: [batch size, src len, enc_hid_dim * 2]\n",
        "        weighted = torch.bmm(a, encoder_outputs)  # weighted shape: [batch size, 1, enc_hid_dim * 2]\n",
        "        weighted = weighted.permute(1, 0, 2)  # weighted shape: [1, batch size, enc_hid_dim * 2]\n",
        "        rnn_input = torch.cat((embedded, weighted), dim=2)  # rnn_input shape: [1, batch size, (enc_hid_dim * 2) + emb_dim]\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        embedded = embedded.squeeze(0)  # embedded shape: [batch size, emb dim]\n",
        "        output = output.squeeze(0)  # output shape: [batch size, dec_hid_dim]\n",
        "        weighted = weighted.squeeze(0)  # weighted shape: [batch size, enc_hid_dim * 2]\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))  # prediction shape: [batch size, output_dim]\n",
        "        return prediction, hidden.squeeze(0), a.squeeze(1)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        trg_len = trg.shape[0]\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "        input = trg[0,:]\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if random.random() < teacher_forcing_ratio else top1\n",
        "        return outputs\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for i, (src, trg) in enumerate(iterator):\n",
        "        src, trg = src.to(model.device), trg.to(model.device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Ensure all indices are within the valid range\n",
        "        src = torch.clamp(src, 0, INPUT_DIM - 1)\n",
        "        trg = torch.clamp(trg, 0, OUTPUT_DIM - 1)\n",
        "\n",
        "        output = model(src, trg)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def accuracy(output, target):\n",
        "    _, predicted = torch.max(output, dim=1)\n",
        "    correct = (predicted == target).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    total_acc = 0\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for src, trg in dataloader:\n",
        "            src = src.permute(1, 0).to(device)\n",
        "            trg = trg.permute(1, 0).to(device)\n",
        "            output = model(src, trg, 0)  # no teacher forcing\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "            loss = criterion(output, trg)\n",
        "            acc = accuracy(output, trg)\n",
        "            total_loss += loss.item()\n",
        "            total_acc += acc.item()\n",
        "    return total_loss / len(dataloader), total_acc / len(dataloader)\n",
        "\n",
        "# Example batch size\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Custom collate function for DataLoader\n",
        "def collate_fn(batch):\n",
        "    srcs = [torch.tensor(item[0].copy()) for item in batch]\n",
        "    trgs = [torch.tensor(item[1].copy()) for item in batch]\n",
        "    srcs = pad_sequence(srcs, batch_first=True, padding_value=0)\n",
        "    trgs = pad_sequence(trgs, batch_first=True, padding_value=0)\n",
        "    return srcs, trgs\n",
        "\n",
        "# Example dataset\n",
        "# Replace with your actual dataset\n",
        "class ExampleDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self):\n",
        "        self.data = [\n",
        "            (np.array([1, 2, 3]), np.array([1, 2, 3])),\n",
        "            (np.array([4, 5]), np.array([4, 5]))\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "train_data = ExampleDataset()\n",
        "\n",
        "# DataLoader instantiation\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "# Define your vocabulary here\n",
        "vocab = {'<PAD>': 0, '<SOS>': 1, '}': 2}\n",
        "\n",
        "INPUT_DIM = len(vocab)\n",
        "OUTPUT_DIM = len(vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, ENC_DROPOUT)\n",
        "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Training loop\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, CLIP) # Use train_loader instead of train_dataloader\n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKsrc05L5MX9",
        "outputId": "97d01099-16d2-4caf-883c-a2e4792db57d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Train Loss: 1.053\n",
            "Epoch: 02, Train Loss: 0.478\n",
            "Epoch: 03, Train Loss: 0.274\n",
            "Epoch: 04, Train Loss: 0.112\n",
            "Epoch: 05, Train Loss: 0.015\n",
            "Epoch: 06, Train Loss: 0.004\n",
            "Epoch: 07, Train Loss: 0.001\n",
            "Epoch: 08, Train Loss: 0.000\n",
            "Epoch: 09, Train Loss: 0.000\n",
            "Epoch: 10, Train Loss: 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Plot Loss Curves and Performance Metrics\n",
        "We'll visualize the loss curves and other metrics."
      ],
      "metadata": {
        "id": "6Opy-sQc4Wkq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u2g7B9jP4cP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        return F.softmax(attention, dim=1)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim, bidirectional=True)\n",
        "        self.fc = nn.Linear(hid_dim * 2, hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
        "        return outputs, hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        input = input.unsqueeze(0)  # input shape: [1, batch size]\n",
        "        embedded = self.dropout(self.embedding(input))  # embedded shape: [1, batch size, emb dim]\n",
        "        a = self.attention(hidden, encoder_outputs)  # attention shape: [batch size, src len]\n",
        "        a = a.unsqueeze(1)  # attention shape: [batch size, 1, src len]\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # encoder_outputs shape: [batch size, src len, enc_hid_dim * 2]\n",
        "        weighted = torch.bmm(a, encoder_outputs)  # weighted shape: [batch size, 1, enc_hid_dim * 2]\n",
        "        weighted = weighted.permute(1, 0, 2)  # weighted shape: [1, batch size, enc_hid_dim * 2]\n",
        "        rnn_input = torch.cat((embedded, weighted), dim=2)  # rnn_input shape: [1, batch size, (enc_hid_dim * 2) + emb_dim]\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        embedded = embedded.squeeze(0)  # embedded shape: [batch size, emb dim]\n",
        "        output = output.squeeze(0)  # output shape: [batch size, dec_hid_dim]\n",
        "        weighted = weighted.squeeze(0)  # weighted shape: [batch size, enc_hid_dim * 2]\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))  # prediction shape: [batch size, output_dim]\n",
        "        return prediction, hidden.squeeze(0), a.squeeze(1)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        trg_len = trg.shape[0]\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "        input = trg[0,:]\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if random.random() < teacher_forcing_ratio else top1\n",
        "        return outputs\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for i, (src, trg) in enumerate(iterator):\n",
        "        src, trg = src.to(model.device), trg.to(model.device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Ensure all indices are within the valid range\n",
        "        src = torch.clamp(src, 0, INPUT_DIM - 1)\n",
        "        trg = torch.clamp(trg, 0, OUTPUT_DIM - 1)\n",
        "\n",
        "        output = model(src, trg)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def accuracy(output, target):\n",
        "    _, predicted = torch.max(output, dim=1)\n",
        "    correct = (predicted == target).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "def evaluate_model(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_acc = 0\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for src, trg in dataloader:\n",
        "            src = src.permute(1, 0).to(device)\n",
        "            trg = trg.permute(1, 0).to(device)\n",
        "\n",
        "            # Ensure all indices are within the valid range\n",
        "            src = torch.clamp(src, 0, INPUT_DIM - 1)\n",
        "            trg = torch.clamp(trg, 0, OUTPUT_DIM - 1)\n",
        "\n",
        "            output = model(src, trg, 0)  # no teacher forcing\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "\n",
        "            # Use .reshape() instead of .view() to handle non-contiguous tensors\n",
        "            trg = trg[1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            acc = accuracy(output, trg)\n",
        "            total_loss += loss.item()\n",
        "            total_acc += acc.item()\n",
        "    return total_loss / len(dataloader), total_acc / len(dataloader)\n",
        "\n",
        "\n",
        "# Example batch size\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Custom collate function for DataLoader\n",
        "def collate_fn(batch):\n",
        "    srcs = [torch.tensor(item[0].copy()) for item in batch]\n",
        "    trgs = [torch.tensor(item[1].copy()) for item in batch]\n",
        "    srcs = pad_sequence(srcs, batch_first=True, padding_value=0)\n",
        "    trgs = pad_sequence(trgs, batch_first=True, padding_value=0)\n",
        "    return srcs, trgs\n",
        "\n",
        "# Example dataset\n",
        "# Replace with your actual dataset\n",
        "class ExampleDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self):\n",
        "        self.data = [\n",
        "            (np.array([1, 2, 3]), np.array([1, 2, 3])),\n",
        "            (np.array([4, 5]), np.array([4, 5]))\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "train_data = ExampleDataset()\n",
        "\n",
        "# DataLoader instantiation\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "# Define your vocabulary here\n",
        "vocab = {'<PAD>': 0, '<SOS>': 1, '}': 2}\n",
        "\n",
        "INPUT_DIM = len(vocab)\n",
        "OUTPUT_DIM = len(vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, ENC_DROPOUT)\n",
        "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "    val_loss, _ = evaluate_model(model, val_loader, criterion)\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Val Loss: {val_loss:.3f}')\n",
        "\n",
        "# Plotting loss curves\n",
        "def plot_loss_curve(train_losses, val_losses):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Loss Curves')\n",
        "    plt.show()\n",
        "\n",
        "plot_loss_curve(train_losses, val_losses)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "test_loss, test_acc = evaluate_model(model, test_loader, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Accuracy: {test_acc*100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "LMiDskYNBHDr",
        "outputId": "d2338c44-8400-4af7-ae97-9e852b4996cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Train Loss: 1.069, Val Loss: 0.443\n",
            "Epoch: 02, Train Loss: 0.386, Val Loss: 0.394\n",
            "Epoch: 03, Train Loss: 0.234, Val Loss: 1.110\n",
            "Epoch: 04, Train Loss: 0.124, Val Loss: 1.394\n",
            "Epoch: 05, Train Loss: 0.012, Val Loss: 1.481\n",
            "Epoch: 06, Train Loss: 0.001, Val Loss: 1.531\n",
            "Epoch: 07, Train Loss: 0.000, Val Loss: 1.566\n",
            "Epoch: 08, Train Loss: 0.000, Val Loss: 1.616\n",
            "Epoch: 09, Train Loss: 0.000, Val Loss: 1.673\n",
            "Epoch: 10, Train Loss: 0.000, Val Loss: 1.733\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.733 | Test Accuracy: 75.00%\n"
          ]
        }
      ]
    }
  ]
}